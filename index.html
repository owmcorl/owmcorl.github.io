<head>
	<title>Offline World Models</title>
	<meta property="og:title" content="Offline World Models">
	<meta property="og:description" content="Offline World Models">
	<link rel="stylesheet" href="style.css">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro">
	<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
</head>
<div class="header" id="top">
	<h1>Finetuning Offline World Models in the Real World</h1>
	<table class="authors">
		<tbody>
			<tr>
				<td>
					<h4>
						<span>Paper ID #57</span><br/><br/>
						<a href="#" class="nobreak">Anonymous Authors</a><br/>
						<span class="authors-affiliation">Anonymous Affiliation</span><br/>
					</h4>
				</td>
			</tr>
		</tbody>
	</table>
	<div class="links">
		<a href="#" class="btn btn-disabled"><i class="fa">&#xf1c1;</i>&ensp;Paper (coming soon)</a>&emsp;<a href="#" class="btn btn-disabled"><i class="fa fa-github"></i>&ensp;Code (coming soon)</a>
	</div>
</div>
<div class="content">
	<div class="figure" style="height: 288px; background-image: url(images/teaser.png);"></div>
	<div style="margin: auto; margin-top: -24px;">
		<p>
			<span class="bold">Approach.</span> We propose a framework for offline pretraining and online finetuning of world models directly in the real world, without reliance on simulators or synthetic data. Our method iteratively collects new data by <span class="highlight">planning with the learned model</span>, and <span class="highlight blue">finetunes the model</span> on a combination of pre-existing data and newly collected data. Our method can be finetuned few-shot on unseen task variations in <span class="bold">â‰¤20 trials</span> by leveraging novel <span class="italic">test-time</span> regularization during planning.
		</p>
	</div>
	<div class="hr"></div>
	<div>
		<h2>Tasks</h2>
		<div class="figure" style="height: 256px; background-image: url(images/tasks.png);"></div>
		<div class="figure-caption" style="margin-top: -16px;">
			<p>
				<span class="bold">Tasks.</span> We consider diverse tasks in simulation (D4RL, xArm) and on a real robot. Our real-world tasks use raw pixels as input. Our method achieves high success rates in offline-to-online transfer to both seen and unseen tasks in just 20 online trials on a real robot.
			</p>
		</div>
	</div>
	<div class="hr"></div>
	<div>
		<h2>Qualitative Results</h2>
		<div class="figure-caption">
			<p>
				<span class="bold">Videos.</span> Our method can be finetuned few-shot to unseen task variations by online RL, directly in the real world. Inputs are raw RGB images and we use sparse rewards. Below videos are generated by our method after just <span class="bold">20</span> trials.
			</p>
		</div>
		<div class="content-video" style="margin-bottom: 48px">
			<div class="content-video-container">
				<div>
					<span class="bold">Reach</span><br/>
				</div>
				<video playsinline="" autoplay="" loop="" preload="" muted="" width="20%">
					<source src="videos/reach0.mp4" type="video/mp4"/>
				</video>
				<video playsinline="" autoplay="" loop="" preload="" muted="" width="20%">
					<source src="videos/reach1.mp4" type="video/mp4"/>
				</video>
				<video playsinline="" autoplay="" loop="" preload="" muted="" width="20%">
					<source src="videos/reach2.mp4" type="video/mp4"/>
				</video><br/><br/>
				<div>
					<span class="bold">Pick</span><br/>
				</div>
				<video playsinline="" autoplay="" loop="" preload="" muted="" width="20%">
					<source src="videos/pick0.mp4" type="video/mp4"/>
				</video>
				<video playsinline="" autoplay="" loop="" preload="" muted="" width="20%">
					<source src="videos/pick1.mp4" type="video/mp4"/>
				</video>
				<video playsinline="" autoplay="" loop="" preload="" muted="" width="20%">
					<source src="videos/pick2.mp4" type="video/mp4"/>
				</video>
			</div>
		</div>
	</div>
	<div class="hr"></div>
	<div>
		<h2>Few-shot Finetuning to Unseen Task Variations</h2>
		<div class="figure-caption">
			<p>
				<span class="bold">Raw training footage.</span> The videos below contain raw training footage of our method being finetuned with online RL to an unseen task variation (20 trials). While zero-shot transfer fails due to a domain gap, we observe a noticable improvement after a handful of trials. Playback speed has been increased for better viewing.
			</p>
		</div>
		<div class="content-video" style="margin-bottom: 48px">
			<div class="content-video-container">
				<video playsinline="" autoplay="" loop="" preload="" muted="" width="27.5%" style="margin-right: 48px">
					<source src="videos/reach3.mp4" type="video/mp4"/>
				</video>
				<video playsinline="" autoplay="" loop="" preload="" muted="" width="27.5%">
					<source src="videos/lift3.mp4" type="video/mp4"/>
				</video>
			</div>
		</div>
	</div>
	<div class="hr"></div>
	<div>
		<h2>Quantitative Results</h2>
		<div class="figure" style="height: 256px; background-image: url(images/results.png);"></div>
		<div class="figure-caption" style="margin-top: -16px;">
			<p>
				<span class="bold">Results.</span> Our method significantly improves the performance of offline-to-online finetuning of world models, and achieves high task success rates in both seen and unseen task variations with as little as 20 online trials on a real robot.
			</p>
		</div>
	</div>
</div>
<footer>
<a href="#top" class="bold">To top &UpArrow;</a>
</footer>
